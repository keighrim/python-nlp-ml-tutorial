{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a8b3fe10-9df9-43f0-a759-51ba44f91bcc"
    }
   },
   "source": [
    "# COSI140b: A Brief Tutorial on Machine Learning with NLTK/Python\n",
    "Keigh Rim, 4/21/2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This is a tutorial on machine learning using NLTK in Python. It is based on Python 2.7 and NLTK3. \n",
    "\n",
    "The tutorial is about \n",
    "* reparing proper representation of data for machine learning\n",
    "* training machine learning algorithms in NLTK\n",
    "* testing the trained algorithms \n",
    "\n",
    "It is not about \n",
    "* engineering features\n",
    "* theoretical aspects of machine learning\n",
    "\n",
    "Now, let's start with importing nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a84f544b-28d7-49d7-b6a6-c8e94072ed47"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aa7133ef-39aa-41ac-84b3-9f6b57f349f1"
    }
   },
   "source": [
    "## Supervised Classification\n",
    "\n",
    "Here we will write Python code to train and test out a classifier using NLTK packages.\n",
    "\n",
    "Our example task is to train a Naive Bayes classifier that predicts gender of the author given a blog post. \n",
    "As our dataset, we will use [`blog-gender-dataset`](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). For details, refer to the paper;\n",
    "* Mukherjee, A., & Liu, B. (2010, October). Improving gender classification of blog authors. In Proceedings of the 2010 conference on Empirical Methods in natural Language Processing (pp. 207-217). Association for Computational Linguistics.\n",
    "\n",
    "In the dataset, we have around 3,000 blog posts, their raw unicode texts and their labels ('F', 'M') indicating the authors' gender. Let's take a peek at it. (The file is in CSV format.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "229d55e2-a5c3-4d73-8269-fdea69c02ada"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('blog-gender-dataset.csv') as dataset: \n",
    "    for i, row in enumerate(csv.reader(dataset)):\n",
    "        print(row)\n",
    "        if i > 0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f47f8ab9-8772-4288-ad4c-713ad7725d3d"
    }
   },
   "source": [
    "These are our *raw* data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "176ea095-d404-408a-a0d0-b3ee04422c8c"
    }
   },
   "source": [
    "### Feature Representation\n",
    "\n",
    "So, as saw above, our *raw* dataset is simply a set of <**unicode string**, **label**> pairs.\n",
    "\n",
    "Now we are turning this into a <**features**, **label**> set. That is, we will write this piece of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f81cecc9-4117-49d2-af21-53aa7b28672d"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_label(label):\n",
    "    return label.strip().upper()\n",
    "\n",
    "def unicodify(text):\n",
    "    return unicode(text, 'utf-8')\n",
    "    \n",
    "def extract_features(text):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "dataset_file = open('blog-gender-dataset.csv')\n",
    "raw_data = [(unicodify(text), normalize_label(label)) \n",
    "            for text, label in csv.reader(dataset_file) \n",
    "            if label]  # some items are missing label\n",
    "dataset_file.close()\n",
    "# consider lazy evaluation for the real implementation\n",
    "\n",
    "feature_representation = {extract_features(text) : label \n",
    "                              for text, label in raw_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ec3effbc-3f0a-49ce-b82a-62c46ef9fd86"
    }
   },
   "source": [
    "#### Defining features\n",
    "Then, what is a feature? A feature is simply a fragmentary/single-faceted description of an instance/item in the data. It can be anything, for example: \n",
    "* *Does the document have a proper name in it? - **Yes***  (Binary feature)\n",
    "* *How many characters are appearing in the document? - **3***  (Discrete numerical feature)\n",
    "* *What is the most frequent non-stopword in the document? - **'time'*** (Nominal feature)\n",
    "\n",
    "A set of features is exactly the way we want to describe an item. And the set of feature values for an item is the representation of that item in the model we design. Thus, after all, we are transforming a document into a set of name-value pairs.\n",
    "\n",
    "However, to statistically model the data, each description (feature value) **needs to be a number**! Binary and numerical features are inherently convertible to numbers. Nominal or ordinal features can be mapped to numbers using random variables. Designing such random variables is beyond the scope of this tutorial and, for the sake of simplicity, we will be using only binary features in the rest of this section. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2fab1431-116c-425a-8a7f-79a9c5e585b0"
    }
   },
   "source": [
    "#### Getting feature values\n",
    "Next question: how can we get values of the features? For instance, say we have a binary feature that represents whether a document contains the word 'time'. How do we get the value for it? Let's take this feature function as an answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "40436eb7-b666-4fb5-ad4c-dc3351a51b1e"
    }
   },
   "outputs": [],
   "source": [
    "def has_time(document):\n",
    "    # returns a feature (yes | no) value directly\n",
    "    return 'time' in document.split()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1682746b-e5e0-4024-a32c-2ac4b60d7869"
    }
   },
   "source": [
    "Besides of the `has_time()` function, suppose we also have `has_flies()`, `has_like()`, `has_an()`, and `has_arrow()` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "9eb5f9b1-757a-45f2-a6d4-3c9493f081de"
    }
   },
   "outputs": [],
   "source": [
    "def has_flies(document):\n",
    "    return 'flies' in document.split()\n",
    "def has_like(document):\n",
    "    return 'like' in document.split()\n",
    "def has_an(document):\n",
    "    return 'an' in document.split()\n",
    "def has_arrow(document):\n",
    "    return 'arrow' in document.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9a00e684-1e7d-4847-809f-2f253d63216d"
    }
   },
   "source": [
    "Then these two short documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document1 = 'time flies like an arrow'\n",
    "document2 = 'time flies when you are having fun'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be transformed into feature representations, or projected into the feature space, like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6141d90b-e2b0-47c9-873b-8b2da2d0d507"
    }
   },
   "outputs": [],
   "source": [
    "doc1_features = {'has_time' : has_time(document1) ,\n",
    "                'has_flies' : has_flies(document1) ,\n",
    "                'has_like' : has_like(document1) ,\n",
    "                'has_an' : has_an(document1) ,\n",
    "                'has_arrow' : has_arrow(document1)}\n",
    "doc2_features = {'has_time' : has_time(document2) ,\n",
    "                'has_flies' : has_flies(document2) ,\n",
    "                'has_like' : has_like(document2) ,\n",
    "                'has_an' : has_an(document2) ,\n",
    "                'has_arrow' : has_arrow(document2)}\n",
    "print 'document1: ', doc1_features\n",
    "print 'document2: ', doc2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4bad6ed0-a8dd-4636-8013-7e94c816dc57"
    }
   },
   "source": [
    "Then, we can factor out the common \"extracting\" procedure as our `extract_features()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "3987e0ed-ca3b-4d11-a1df-961b78c52a8e"
    }
   },
   "outputs": [],
   "source": [
    "feature_functions = [has_time, \n",
    "                     has_flies, \n",
    "                     has_like, \n",
    "                     has_an, \n",
    "                     has_arrow ] \n",
    "\n",
    "def extract_features(document):\n",
    "    return {feature_function.__name__: feature_function(document) \n",
    "            for feature_function in feature_functions}\n",
    "\n",
    "print 'document1: ', extract_features(document1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that each feature function here returns the value of the feature directly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e8503d51-a923-497e-b4c2-76545761c1f4"
    }
   },
   "source": [
    "Okay, that was a silly example, now let's consider a more realistic example using `blog-gender-dataset`. \n",
    "\n",
    "Let's say we want to use all the unigrams as our features, using simple whitespace tokenization. Then we are going to have ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "970b6c27-e6ca-48be-aa6d-5737031c68bf"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for document, _ in raw_data:\n",
    "    vocabulary.update(document.split())\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d39920e0-2495-4225-864d-6ec98b269bbe"
    }
   },
   "source": [
    "Yup, we are going to have 128k features. So, do we need to write .1 million feature functions? Of course not. We can re-write the `extract_features()` function, to avoid writing 128k other functions, like such: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "10dc3e2c-f4d1-4483-99bc-82f7505041e4"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    # now each feature_function can handle multiple features\n",
    "    # and should return the values wrapped in a dict\n",
    "    features = {}\n",
    "    for feature_function in feature_functions:\n",
    "        features.update(feature_function(document))  \n",
    "    return features\n",
    "\n",
    "def unigram_bow(document):\n",
    "    return {token: (token in document.split()) for token in vocabulary}\n",
    "    \n",
    "feature_functions = [unigram_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fd35bc0c-e264-463d-8cbf-338cd26ee8c4"
    }
   },
   "source": [
    "Now, how can you write another feature function `bigram_bow()` and incorporate it into `extract_features()`? And then how many features are we going to have? If we iterate through all three thousands of documents in the dataset and compute all the features, how long would it take? Can you write something in a more efficient way? For example, are all the word types including stopwords in the corpus going to be helpful features? How about stemming and lemmaization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "86969424-220e-4c45-9881-171d8aed5ab0"
    }
   },
   "source": [
    "#### Feature representation for NLTK classifiers\n",
    "Naive Bayes and MaxEnt in NLTK takes a list of tuples (feature_representation, label) as a training set. And they are smart enough to automatically take non-specified features as having *null*  values (`False`, `None`, `0`, etc) by default. As a result, for example, to get `unigram_bow()` features, we don't need to iterate over the whole vocabulary over and over. Rather, we re-write the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d46da52b-954d-48c9-932a-cef0135b29a2"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def unigram_bow_rewrite(document):\n",
    "    return {token: True for token in document.split()}\n",
    "\n",
    "\n",
    "feature_functions = [unigram_bow_rewrite]\n",
    "print(extract_features(raw_data[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4cbf07ef-fb70-4dd8-b759-6e2a56dd7078"
    }
   },
   "source": [
    "Putting all together, we finally translate the entire dataset into this `feature_representation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1eec711b-475b-4962-9d0c-8cc81ee8b43d"
    }
   },
   "outputs": [],
   "source": [
    "feature_representation = [(extract_features(raw_text), label) \n",
    "                          for raw_text, label in raw_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "68b0ffb1-94c8-43cd-a552-3b01c5972892"
    }
   },
   "source": [
    "Lastly, let's split the dataset into the training set and the test set for testing the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "99f26c11-9a6e-441e-9bcc-47740beba10a"
    }
   },
   "outputs": [],
   "source": [
    "testset_percentage = 10\n",
    "\n",
    "def split_dataset(dataset, testset_percentage):\n",
    "    cutoff = testset_percentage * len(dataset) / 100\n",
    "    return dataset[cutoff:], dataset[:cutoff]\n",
    "\n",
    "trainset, testset = split_dataset(feature_representation, \n",
    "                                  testset_percentage)\n",
    "\n",
    "print len(trainset)\n",
    "print len(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e0b14d04-a7e5-48d0-8fe7-7bf23bbaa555"
    }
   },
   "source": [
    "### Training classifiers\n",
    "\n",
    "Once we have the feature representation, training a classifier is straightforward. We only need to call `train()` method on our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "953dcab7-f346-4f53-b26f-48a21aa2fd32"
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier = nltk.classify.NaiveBayesClassifier.train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f7507228-6abb-4012-8cf7-7d3b5c36cdca"
    }
   },
   "source": [
    "Other than Naive Bayes, NLTK comes with MaxEnt, decision tree, and many other classifier implementations. For details, refer to [the official documentation](http://www.nltk.org/api/nltk.classify.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fc0920ea-60fe-4af5-b9ce-cac9ab2bf470"
    }
   },
   "source": [
    "### Testing a classifier\n",
    "\n",
    "Now let's see how good our naive bayes classifier is. After training, the classifier can perform classification upon `classify()` calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "96d9d1f8-5fa8-4616-848f-411b48f670b9"
    }
   },
   "outputs": [],
   "source": [
    "predictions = [nb_classifier.classify(test_document) \n",
    "               for test_document, _ in testset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aa48670a-3deb-4759-afba-eef01cce8e87"
    }
   },
   "source": [
    "We can measure its performance on many different aspect. First, let's see its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "44934939-4d20-4adf-9a4e-22f93ce2b2a3"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, golds):\n",
    "    return (len([p for p, g in zip(predictions, golds) if p == g]) \n",
    "            / float(len(golds)))\n",
    "\n",
    "golds = [label for _ , label in testset]\n",
    "print(accuracy(predictions, golds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "962e0ea6-2840-4824-817a-b2bfcda372b3"
    }
   },
   "source": [
    "To compute precision and recall, NLTK provides a convenient method to draw a confusion matrix as well as computation of precision and recall. However, we need a different data structure; sets of document id's of different labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7a194a64-3eb4-4eb7-b87e-a8e7b0dbb5f0"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict as ddict\n",
    "\n",
    "def to_labeled_ids(labeled_data):\n",
    "    d = ddict(set)\n",
    "    for doc_id, label in enumerate(labeled_data):\n",
    "        d[label].add(doc_id)\n",
    "    return d\n",
    "\n",
    "def print_clf_scores(gld, hyp):\n",
    "    print nltk.ConfusionMatrix(gld, hyp)\n",
    "\n",
    "    gld_sets = to_labeled_ids(gld)\n",
    "    hyp_sets = to_labeled_ids(hyp)\n",
    "\n",
    "    from nltk.metrics import scores\n",
    "    for label in set(gld):\n",
    "        r = scores.recall(gld_sets[label], hyp_sets[label])\n",
    "        p = scores.precision(gld_sets[label], hyp_sets[label])\n",
    "        f = scores.f_measure(gld_sets[label], hyp_sets[label])\n",
    "        print('<{}> P: {:.2}, R: {:.2}, F: {:.2}'.format(label, p, r, f))\n",
    " \n",
    "print_clf_scores(golds, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6bf1e007-0b8c-4523-93b7-c9b5a96f91b5"
    }
   },
   "source": [
    "Lastly, but definately not least, NLTK provides a method to rank the helpfulness of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6288965c-95de-42b6-8e74-f27029013f19"
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `scikit-learn` wrapper in `NLTK`\n",
    "\n",
    "NLTK also has a wrapper class that wraps around `scikit-learn` classifiers. Here's an example of using a support vector classifier from scikit-learn (`sklearn.svm.SVC`). Note that you should have `scikit-learn` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "linearsvm_clf = SklearnClassifier(SVC(kernel='linear')).train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way it wraps a `scikit-learn` classifier is pretty straightforward: 1) import the classifier (*in scikit-learn, they use a term `estimator`*) class, 2) initiate an instance of it, and 3) wrap it in `nltk.classify.scikitlearn.SklearnClassifier` class. \n",
    "\n",
    "Now let's have another SVM using a different kernel function, just for fun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radialsvm_clf = SklearnClassifier(SVC(kernel='rbf')).train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how they works. Note here we use `classifiy_many` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_predictions = [clf.classify_many([test_doc for test_doc, _ in testset]) \n",
    "                   for clf in [linearsvm_clf, radialsvm_clf]]\n",
    "print(accuracy(svm_predictions[0], golds))\n",
    "print(accuracy(svm_predictions[1], golds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "abca40b7-65d7-48b8-a686-dd3b7775513b"
    }
   },
   "source": [
    "So far, we have seen how to write Python code to transform documents into feature sets and to train/test a classifier for a simple classification problem. Now let's take a look at different types of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "97b37cf3-b979-46c1-bd51-865a1dff9417"
    }
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7053cc15-1592-4494-adb8-570e340f14fd"
    }
   },
   "source": [
    "### k_means clustering\n",
    "\n",
    "NLTK clustering implementations requires a **complete vector representation** (using `numpy.ndarray`) of the corpus, not a `dict` based featuresets.\n",
    "\n",
    "Let's do this with 128k unigram features, which will result in a very sparse feature vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b67074a4-1274-4079-802d-976be3c44a7e"
    }
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "feature_indices = list(vocabulary) # we need an indexed feature names\n",
    "\n",
    "feature_vectors = []   # this will be a #documents X #features matrix\n",
    "                       # a huge and sparse one\n",
    "    \n",
    "def generate_document_vector(document):\n",
    "    document_vector = numpy.zeros(len(feature_indices))\n",
    "    for word_type in set(document.split()):\n",
    "        document_vector[feature_indices.index(word_type)] = 1\n",
    "    return document_vector\n",
    "\n",
    "feature_vectors = [generate_document_vector(document) \n",
    "                   for document, _ in raw_data[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0088580d-0926-44bd-9edb-a4bf4ca4d7ae"
    }
   },
   "source": [
    "By the way, [`numpy.ndarray`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.ndarray.html) is a matrix-like python object that is highly optimized for numerical computation. Linear algebraic operations, such as arithmetic on matrices or matrix manipulations, are super fast with ndarrays, outperforming list comprehensions in Python with a huge gap (not to mention looping), so most scientific packages in Python including all widely-used machine learning packages (scikit-learn, theano, tensorflow, you name it) are heavily relying on `numpy` data structures and functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c6effe0-db1a-45aa-be57-dfddf1f4c47e"
    }
   },
   "source": [
    "Next, we will build 2 clusters using k-means algorithm based on cosine similarity between these vectors. Note that the k-means algorithm starts with random seeds and does not guarantee the global convergence. Thus, one might want to repeat the algorithm then take the most common result as a 'good enough' clustering (by using `repeats` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "a67c4a43-c24a-4a41-a16d-839336383be7"
    }
   },
   "outputs": [],
   "source": [
    "dist = nltk.cluster.cosine_distance\n",
    "kmc = nltk.cluster.kmeans.KMeansClusterer(2, dist, repeats=10)\n",
    "\n",
    "clustered = kmc.cluster(feature_vectors, True)\n",
    "print(clustered)\n",
    "\n",
    "gold_labels = [int(label=='M') for _, label in raw_data[:10]]\n",
    "print(gold_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "34ad5049-6abf-4285-9a76-953950e8c6ca"
    }
   },
   "source": [
    "After trained, the clustering can be used a sort-of classifier, like such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "eb6a2f9e-4105-4763-8557-a6ff4a726abf"
    }
   },
   "outputs": [],
   "source": [
    "kmc.classify(numpy.array(generate_document_vector(raw_data[111][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b827cb5b-c469-47a2-a39b-e005a3567aed"
    }
   },
   "source": [
    "## Supervised Sequencial Tagging\n",
    "\n",
    "### HMM tagger\n",
    "\n",
    "Here, we are going to train a HMM sequence tagger for Chinese word segmentation, using **BIO tagging**. Included in the directory is word segmentation annotation of around 20k Chinese sentences from news articles, a small part of [Chinese Treebank corpus](http://www.cs.brandeis.edu/~clp/ctb/). \n",
    "Let's take a look at the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "671f10ae-a593-4eab-98f4-680a160ef75b"
    }
   },
   "outputs": [],
   "source": [
    "with open('tagged.seg') as seg_annotation:\n",
    "    for i, line in enumerate(seg_annotation):\n",
    "        print line\n",
    "        if i > 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6fb7e50-3b9c-4b25-8193-54630b3fb664"
    }
   },
   "source": [
    "For details for CTB, see the paper: \n",
    "* Xue, N., Xia, F., Chiou, F. D., & Palmer, M. (2005). The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural language engineering, 11(02), 207-238.\n",
    "\n",
    "The original word segmentation annotation is done by simply inserting whitespaces, and I did BIO tagging based on the annotation last night. It was a complete hack job, and all errors in BIO tagging are my fault. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e420cb6-4442-4426-bf33-216b77909be5"
    }
   },
   "source": [
    "To feed the data into supervised training process implemented in NLTK, we need to prepare a sequence of <**observation, state**> tuples. And in this case, our observations will just going to be unicode code points of each character and states are word boundaries encoded with BIO tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "cbc636fc-c930-4f00-a5ad-73f6ef929602"
    }
   },
   "outputs": [],
   "source": [
    "tagged_segmentation = []\n",
    "with open('tagged.seg') as annotation:\n",
    "    for line in annotation:\n",
    "        tagged_segmentation.append(\n",
    "            [tuple(token.split(\"_\")) for token in line.split()])\n",
    "    \n",
    "print(len(tagged_segmentation))\n",
    "print(tagged_segmentation[234])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e8611de6-45e6-42d3-85d3-273f3f71f4c5"
    }
   },
   "source": [
    "Okay, now we have about 20k sequences of word segmentation tagging. As always, training starts with split the data into the train set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "4ac264d1-4cc7-4dfb-93df-c27d2934bde8"
    }
   },
   "outputs": [],
   "source": [
    "trainset, testset = split_dataset(tagged_segmentation, \n",
    "                                  testset_percentage)  # = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ff892f96-d0c5-41dd-ae93-6d6e854de78f"
    }
   },
   "source": [
    "Again, training is fairly straightforward. We need to create an trainer object and then call `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "69307977-186a-4cf6-8c58-2ea57943c23e"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "hmm_tagger = HiddenMarkovModelTrainer().train(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "87be48ce-1847-4501-9c9f-c49c6334ecc0"
    }
   },
   "source": [
    "After all, we now have a tagger that can perform word segmentation on an arbitrary Chinese sentence. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8f170ec9-5ef5-4700-a06a-f98b40a4225f"
    }
   },
   "outputs": [],
   "source": [
    "def glue_chars(chars_seq):\n",
    "    return \"\".join(chars_seq)\n",
    "\n",
    "def bio_to_whtspc(tagged_seq):\n",
    "    return glue_chars([\" \" + char if tag == \"B\" else char \n",
    "                       for char, tag in tagged_seq]).strip()\n",
    "\n",
    "test_sent_idx = 2\n",
    "\n",
    "print \"ori:\", glue_chars([char for char, tag in testset[test_sent_idx]])\n",
    "print \"gld:\", bio_to_whtspc(testset[test_sent_idx])\n",
    "print \"hyp:\", bio_to_whtspc(hmm_tagger.tag([char for char, tag in testset[test_sent_idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3deba89e-1017-4336-bdab-43d35a0998e5"
    }
   },
   "source": [
    "But, how good does it perform in general? We can measure the tagger's accuracy with a simple computation, as we did in the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "75ec1b2b-4d5b-490a-945b-0ca884cc4801"
    }
   },
   "outputs": [],
   "source": [
    "predictions= []\n",
    "gold = []\n",
    "\n",
    "for sent in testset:\n",
    "    predictions.extend([predicted for _, predicted \n",
    "                        in hmm_tagger.tag([char for char, tag in sent])])\n",
    "    gold.extend([gold_tag for _, gold_tag in sent])\n",
    "\n",
    "print \"accuracy:\", accuracy(predictions, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "10a34621-32f5-4cb9-b9f1-6a049c6d2500"
    }
   },
   "source": [
    "As well as confusion matrix, and P/R/F measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "674ff347-73dc-4fba-8f2f-f84e9e7e8d6f"
    }
   },
   "outputs": [],
   "source": [
    "print_clf_scores(gold, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
